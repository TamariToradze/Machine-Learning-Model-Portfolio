{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3816,"databundleVersionId":32105,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-31T20:55:48.068436Z","iopub.execute_input":"2025-07-31T20:55:48.069105Z","iopub.status.idle":"2025-07-31T20:55:48.587938Z","shell.execute_reply.started":"2025-07-31T20:55:48.069074Z","shell.execute_reply":"2025-07-31T20:55:48.586581Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/walmart-recruiting-store-sales-forecasting/train.csv.zip\n/kaggle/input/walmart-recruiting-store-sales-forecasting/sampleSubmission.csv.zip\n/kaggle/input/walmart-recruiting-store-sales-forecasting/stores.csv\n/kaggle/input/walmart-recruiting-store-sales-forecasting/features.csv.zip\n/kaggle/input/walmart-recruiting-store-sales-forecasting/test.csv.zip\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"pip install mlflow==1.30.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T08:50:20.892648Z","iopub.execute_input":"2025-08-01T08:50:20.894132Z","iopub.status.idle":"2025-08-01T08:50:43.107971Z","shell.execute_reply.started":"2025-08-01T08:50:20.894095Z","shell.execute_reply":"2025-08-01T08:50:43.106528Z"}},"outputs":[{"name":"stdout","text":"Collecting mlflow==1.30.0\n  Downloading mlflow-1.30.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==1.30.0) (8.2.1)\nCollecting cloudpickle<3 (from mlflow==1.30.0)\n  Downloading cloudpickle-2.2.1-py3-none-any.whl.metadata (6.9 kB)\nCollecting databricks-cli<1,>=0.8.7 (from mlflow==1.30.0)\n  Downloading databricks_cli-0.18.0-py2.py3-none-any.whl.metadata (4.0 kB)\nRequirement already satisfied: entrypoints<1 in /usr/local/lib/python3.11/dist-packages (from mlflow==1.30.0) (0.4)\nRequirement already satisfied: gitpython<4,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==1.30.0) (3.1.44)\nRequirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.11/dist-packages (from mlflow==1.30.0) (6.0.2)\nRequirement already satisfied: protobuf<5,>=3.12.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==1.30.0) (3.20.3)\nCollecting pytz<2023 (from mlflow==1.30.0)\n  Downloading pytz-2022.7.1-py2.py3-none-any.whl.metadata (21 kB)\nRequirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.11/dist-packages (from mlflow==1.30.0) (2.32.4)\nCollecting packaging<22 (from mlflow==1.30.0)\n  Downloading packaging-21.3-py3-none-any.whl.metadata (15 kB)\nCollecting importlib-metadata!=4.7.0,<6,>=3.7.0 (from mlflow==1.30.0)\n  Downloading importlib_metadata-5.2.0-py3-none-any.whl.metadata (5.0 kB)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow==1.30.0) (0.5.3)\nRequirement already satisfied: alembic<2 in /usr/local/lib/python3.11/dist-packages (from mlflow==1.30.0) (1.16.2)\nCollecting docker<7,>=4.0.0 (from mlflow==1.30.0)\n  Downloading docker-6.1.3-py3-none-any.whl.metadata (3.5 kB)\nCollecting Flask<3 (from mlflow==1.30.0)\n  Downloading flask-2.3.3-py3-none-any.whl.metadata (3.6 kB)\nRequirement already satisfied: numpy<2 in /usr/local/lib/python3.11/dist-packages (from mlflow==1.30.0) (1.26.4)\nRequirement already satisfied: scipy<2 in /usr/local/lib/python3.11/dist-packages (from mlflow==1.30.0) (1.15.3)\nCollecting pandas<2 (from mlflow==1.30.0)\n  Downloading pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nCollecting prometheus-flask-exporter<1 (from mlflow==1.30.0)\n  Downloading prometheus_flask_exporter-0.23.2-py3-none-any.whl.metadata (20 kB)\nCollecting querystring-parser<2 (from mlflow==1.30.0)\n  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl.metadata (559 bytes)\nCollecting sqlalchemy<2,>=1.4.0 (from mlflow==1.30.0)\n  Downloading SQLAlchemy-1.4.54-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\nCollecting gunicorn<21 (from mlflow==1.30.0)\n  Downloading gunicorn-20.1.0-py3-none-any.whl.metadata (3.8 kB)\nRequirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic<2->mlflow==1.30.0) (1.3.10)\nRequirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic<2->mlflow==1.30.0) (4.14.0)\nRequirement already satisfied: pyjwt>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from databricks-cli<1,>=0.8.7->mlflow==1.30.0) (2.10.1)\nRequirement already satisfied: oauthlib>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from databricks-cli<1,>=0.8.7->mlflow==1.30.0) (3.3.1)\nRequirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.11/dist-packages (from databricks-cli<1,>=0.8.7->mlflow==1.30.0) (0.9.0)\nRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from databricks-cli<1,>=0.8.7->mlflow==1.30.0) (1.17.0)\nRequirement already satisfied: urllib3<3,>=1.26.7 in /usr/local/lib/python3.11/dist-packages (from databricks-cli<1,>=0.8.7->mlflow==1.30.0) (2.5.0)\nRequirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from docker<7,>=4.0.0->mlflow==1.30.0) (1.8.0)\nRequirement already satisfied: Werkzeug>=2.3.7 in /usr/local/lib/python3.11/dist-packages (from Flask<3->mlflow==1.30.0) (3.1.3)\nRequirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3->mlflow==1.30.0) (3.1.6)\nRequirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3->mlflow==1.30.0) (2.2.0)\nRequirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3->mlflow==1.30.0) (1.9.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython<4,>=2.1.0->mlflow==1.30.0) (4.0.12)\nRequirement already satisfied: setuptools>=3.0 in /usr/local/lib/python3.11/dist-packages (from gunicorn<21->mlflow==1.30.0) (75.2.0)\nRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata!=4.7.0,<6,>=3.7.0->mlflow==1.30.0) (3.23.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2->mlflow==1.30.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2->mlflow==1.30.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2->mlflow==1.30.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2->mlflow==1.30.0) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2->mlflow==1.30.0) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2->mlflow==1.30.0) (2.4.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from packaging<22->mlflow==1.30.0) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from pandas<2->mlflow==1.30.0) (2.9.0.post0)\nRequirement already satisfied: prometheus_client in /usr/local/lib/python3.11/dist-packages (from prometheus-flask-exporter<1->mlflow==1.30.0) (0.22.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow==1.30.0) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow==1.30.0) (3.10)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow==1.30.0) (2025.6.15)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<2,>=1.4.0->mlflow==1.30.0) (3.2.3)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=2.1.0->mlflow==1.30.0) (5.0.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->Flask<3->mlflow==1.30.0) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2->mlflow==1.30.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2->mlflow==1.30.0) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2->mlflow==1.30.0) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2->mlflow==1.30.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2->mlflow==1.30.0) (2024.2.0)\nDownloading mlflow-1.30.0-py3-none-any.whl (17.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\nDownloading databricks_cli-0.18.0-py2.py3-none-any.whl (150 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.3/150.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading docker-6.1.3-py3-none-any.whl (148 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.1/148.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading flask-2.3.3-py3-none-any.whl (96 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.1/96.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading importlib_metadata-5.2.0-py3-none-any.whl (21 kB)\nDownloading packaging-21.3-py3-none-any.whl (40 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading prometheus_flask_exporter-0.23.2-py3-none-any.whl (19 kB)\nDownloading pytz-2022.7.1-py2.py3-none-any.whl (499 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.4/499.4 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\nDownloading SQLAlchemy-1.4.54-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pytz, sqlalchemy, querystring-parser, packaging, importlib-metadata, gunicorn, cloudpickle, Flask, docker, databricks-cli, prometheus-flask-exporter, pandas, mlflow\n  Attempting uninstall: pytz\n    Found existing installation: pytz 2025.2\n    Uninstalling pytz-2025.2:\n      Successfully uninstalled pytz-2025.2\n  Attempting uninstall: sqlalchemy\n    Found existing installation: SQLAlchemy 2.0.41\n    Uninstalling SQLAlchemy-2.0.41:\n      Successfully uninstalled SQLAlchemy-2.0.41\n  Attempting uninstall: packaging\n    Found existing installation: packaging 25.0\n    Uninstalling packaging-25.0:\n      Successfully uninstalled packaging-25.0\n  Attempting uninstall: importlib-metadata\n    Found existing installation: importlib_metadata 8.7.0\n    Uninstalling importlib_metadata-8.7.0:\n      Successfully uninstalled importlib_metadata-8.7.0\n  Attempting uninstall: cloudpickle\n    Found existing installation: cloudpickle 3.1.1\n    Uninstalling cloudpickle-3.1.1:\n      Successfully uninstalled cloudpickle-3.1.1\n  Attempting uninstall: Flask\n    Found existing installation: Flask 3.1.1\n    Uninstalling Flask-3.1.1:\n      Successfully uninstalled Flask-3.1.1\n  Attempting uninstall: docker\n    Found existing installation: docker 7.1.0\n    Uninstalling docker-7.1.0:\n      Successfully uninstalled docker-7.1.0\n  Attempting uninstall: pandas\n    Found existing installation: pandas 2.2.3\n    Uninstalling pandas-2.2.3:\n      Successfully uninstalled pandas-2.2.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\ndask 2024.12.1 requires cloudpickle>=3.0.0, but you have cloudpickle 2.2.1 which is incompatible.\ndistributed 2024.12.1 requires cloudpickle>=3.0.0, but you have cloudpickle 2.2.1 which is incompatible.\ndask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\ndask-expr 1.1.21 requires pandas>=2, but you have pandas 1.5.3 which is incompatible.\ncudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\nwoodwork 0.31.0 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\nfeaturetools 1.31.0 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\nvisions 0.8.1 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\npyldavis 3.4.1 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.4 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.1 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\npandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\npandas-gbq 0.29.1 requires packaging>=22.0.0, but you have packaging 21.3 which is incompatible.\nipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.54 which is incompatible.\nibis-framework 9.5.0 requires toolz<1,>=0.11, but you have toolz 1.0.0 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\nastropy 7.1.0 requires packaging>=22.0.0, but you have packaging 21.3 which is incompatible.\nlangsmith 0.4.1 requires packaging>=23.2, but you have packaging 21.3 which is incompatible.\nlangchain-core 0.3.66 requires packaging<25,>=23.2, but you have packaging 21.3 which is incompatible.\nmizani 0.13.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\nplotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\ndataproc-spark-connect 0.7.5 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\ndb-dtypes 1.4.3 requires packaging>=24.2.0, but you have packaging 21.3 which is incompatible.\nsphinx 8.2.3 requires packaging>=23.0, but you have packaging 21.3 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nxarray 2025.3.1 requires packaging>=23.2, but you have packaging 21.3 which is incompatible.\nxarray 2025.3.1 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed Flask-2.3.3 cloudpickle-2.2.1 databricks-cli-0.18.0 docker-6.1.3 gunicorn-20.1.0 importlib-metadata-5.2.0 mlflow-1.30.0 packaging-21.3 pandas-1.5.3 prometheus-flask-exporter-0.23.2 pytz-2022.7.1 querystring-parser-1.2.4 sqlalchemy-1.4.54\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install scikit-learn==1.3.2\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T09:08:24.521440Z","iopub.execute_input":"2025-08-01T09:08:24.521870Z","iopub.status.idle":"2025-08-01T09:08:28.765031Z","shell.execute_reply.started":"2025-08-01T09:08:24.521826Z","shell.execute_reply":"2025-08-01T09:08:28.763809Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: scikit-learn==1.3.2 in /usr/local/lib/python3.11/dist-packages (1.3.2)\nRequirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.3.2) (1.26.4)\nRequirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.3.2) (1.15.3)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.3.2) (1.5.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.3.2) (3.6.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.0,>=1.17.3->scikit-learn==1.3.2) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.0,>=1.17.3->scikit-learn==1.3.2) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.0,>=1.17.3->scikit-learn==1.3.2) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.0,>=1.17.3->scikit-learn==1.3.2) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.0,>=1.17.3->scikit-learn==1.3.2) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.0,>=1.17.3->scikit-learn==1.3.2) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0,>=1.17.3->scikit-learn==1.3.2) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.0,>=1.17.3->scikit-learn==1.3.2) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.0,>=1.17.3->scikit-learn==1.3.2) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.0,>=1.17.3->scikit-learn==1.3.2) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.0,>=1.17.3->scikit-learn==1.3.2) (2024.2.0)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n# Install and setup MLflow with DagsHub\n!pip install dagshub mlflow\nimport dagshub\nimport mlflow\nimport mlflow.sklearn\nimport mlflow.lightgbm\nfrom mlflow.models.signature import infer_signature\n\n# Initialize DagsHub and MLflow\ndagshub.init(repo_owner='TamariToradze', repo_name='ML-Final', mlflow=True)\n\n# Data Loading and Preparation\nimport pandas as pd\nimport zipfile\nimport os\n\n# Paths to zip files\nzip_files = [\"/kaggle/input/walmart-recruiting-store-sales-forecasting/features.csv.zip\", \n             \"/kaggle/input/walmart-recruiting-store-sales-forecasting/test.csv.zip\", \n             \"/kaggle/input/walmart-recruiting-store-sales-forecasting/train.csv.zip\"]\n\n# Unzip all files\nfor zip_file in zip_files:\n    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n        zip_ref.extractall(\"data\")\n\n# Load datasets\nfeatures_df = pd.read_csv(\"data/features.csv\")\nstores_df = pd.read_csv(\"/kaggle/input/walmart-recruiting-store-sales-forecasting/stores.csv\")\ntrain_df = pd.read_csv(\"data/train.csv\")\ntest_df = pd.read_csv(\"data/test.csv\")\n\n# Quick data check\nprint(f\"Features Shape: {features_df.shape}\")\nprint(f\"Stores Shape: {stores_df.shape}\")\nprint(f\"Train Shape: {train_df.shape}\")\nprint(f\"Test Shape: {test_df.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T09:18:05.624340Z","iopub.execute_input":"2025-08-01T09:18:05.625247Z","iopub.status.idle":"2025-08-01T09:19:30.584074Z","shell.execute_reply.started":"2025-08-01T09:18:05.625217Z","shell.execute_reply":"2025-08-01T09:19:30.583056Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/walmart-recruiting-store-sales-forecasting/train.csv.zip\n/kaggle/input/walmart-recruiting-store-sales-forecasting/sampleSubmission.csv.zip\n/kaggle/input/walmart-recruiting-store-sales-forecasting/stores.csv\n/kaggle/input/walmart-recruiting-store-sales-forecasting/features.csv.zip\n/kaggle/input/walmart-recruiting-store-sales-forecasting/test.csv.zip\nCollecting dagshub\n  Downloading dagshub-0.6.2-py3-none-any.whl.metadata (12 kB)\nCollecting mlflow\n  Downloading mlflow-3.1.4-py3-none-any.whl.metadata (29 kB)\nRequirement already satisfied: PyYAML>=5 in /usr/local/lib/python3.11/dist-packages (from dagshub) (6.0.2)\nCollecting appdirs>=1.4.4 (from dagshub)\n  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\nRequirement already satisfied: click>=8.0.4 in /usr/local/lib/python3.11/dist-packages (from dagshub) (8.2.1)\nRequirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from dagshub) (0.28.1)\nRequirement already satisfied: GitPython>=3.1.29 in /usr/local/lib/python3.11/dist-packages (from dagshub) (3.1.44)\nRequirement already satisfied: rich>=13.1.0 in /usr/local/lib/python3.11/dist-packages (from dagshub) (14.0.0)\nCollecting dacite~=1.6.0 (from dagshub)\n  Downloading dacite-1.6.0-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: tenacity>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from dagshub) (8.5.0)\nCollecting gql[requests] (from dagshub)\n  Downloading gql-3.5.3-py2.py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from dagshub) (0.6.7)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from dagshub) (2.2.3)\nCollecting treelib>=1.6.4 (from dagshub)\n  Downloading treelib-1.8.0-py3-none-any.whl.metadata (3.3 kB)\nCollecting pathvalidate>=3.0.0 (from dagshub)\n  Downloading pathvalidate-3.3.1-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from dagshub) (2.9.0.post0)\nRequirement already satisfied: boto3 in /usr/local/lib/python3.11/dist-packages (from dagshub) (1.39.1)\nRequirement already satisfied: semver in /usr/local/lib/python3.11/dist-packages (from dagshub) (3.0.4)\nCollecting dagshub-annotation-converter>=0.1.5 (from dagshub)\n  Downloading dagshub_annotation_converter-0.1.11-py3-none-any.whl.metadata (3.2 kB)\nCollecting mlflow-skinny==3.1.4 (from mlflow)\n  Downloading mlflow_skinny-3.1.4-py3-none-any.whl.metadata (30 kB)\nRequirement already satisfied: Flask<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.1)\nRequirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.16.2)\nRequirement already satisfied: docker<8,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (7.1.0)\nCollecting graphene<4 (from mlflow)\n  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\nCollecting gunicorn<24 (from mlflow)\n  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\nRequirement already satisfied: matplotlib<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.7.2)\nRequirement already satisfied: numpy<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.26.4)\nRequirement already satisfied: pyarrow<21,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (19.0.1)\nRequirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.2.2)\nRequirement already satisfied: scipy<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.15.3)\nRequirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.41)\nRequirement already satisfied: cachetools<7,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (5.5.2)\nRequirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (3.1.1)\nCollecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==3.1.4->mlflow)\n  Downloading databricks_sdk-0.61.0-py3-none-any.whl.metadata (39 kB)\nRequirement already satisfied: fastapi<1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (0.115.13)\nRequirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (8.7.0)\nCollecting opentelemetry-api<3,>=1.9.0 (from mlflow-skinny==3.1.4->mlflow)\n  Downloading opentelemetry_api-1.36.0-py3-none-any.whl.metadata (1.5 kB)\nCollecting opentelemetry-sdk<3,>=1.9.0 (from mlflow-skinny==3.1.4->mlflow)\n  Downloading opentelemetry_sdk-1.36.0-py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: packaging<26 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (25.0)\nRequirement already satisfied: protobuf<7,>=3.12.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (3.20.3)\nRequirement already satisfied: pydantic<3,>=1.10.8 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (2.11.7)\nRequirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (2.32.4)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (0.5.3)\nRequirement already satisfied: typing-extensions<5,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (4.14.0)\nRequirement already satisfied: uvicorn<1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.4->mlflow) (0.34.3)\nRequirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.3.10)\nRequirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from dagshub-annotation-converter>=0.1.5->dagshub) (5.4.0)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from dagshub-annotation-converter>=0.1.5->dagshub) (11.2.1)\nRequirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker<8,>=4.0.0->mlflow) (2.5.0)\nRequirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (1.9.0)\nRequirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (2.2.0)\nRequirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.6)\nRequirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.0.2)\nRequirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.3)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from GitPython>=3.1.29->dagshub) (4.0.12)\nCollecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\nCollecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (4.9.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (2025.6.15)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (3.10)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->dagshub) (0.16.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (4.58.4)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.4.8)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (3.0.9)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3->mlflow) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3->mlflow) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3->mlflow) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3->mlflow) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3->mlflow) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3->mlflow) (2.4.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->dagshub) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->dagshub) (2025.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil->dagshub) (1.17.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.1.0->dagshub) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.1.0->dagshub) (2.19.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (1.5.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (3.6.0)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.2.3)\nRequirement already satisfied: botocore<1.40.0,>=1.39.1 in /usr/local/lib/python3.11/dist-packages (from boto3->dagshub) (1.39.1)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3->dagshub) (1.0.1)\nRequirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from boto3->dagshub) (0.13.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->dagshub) (3.26.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->dagshub) (0.9.0)\nRequirement already satisfied: yarl<2.0,>=1.6 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (1.20.1)\nCollecting backoff<3.0,>=1.11.1 (from gql[requests]->dagshub)\n  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: requests-toolbelt<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (1.0.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.23.0->dagshub) (1.3.1)\nRequirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.11/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.4->mlflow) (2.40.3)\nRequirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi<1->mlflow-skinny==3.1.4->mlflow) (0.46.2)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->GitPython>=3.1.29->dagshub) (5.0.2)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.1.4->mlflow) (3.23.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.1.0->dagshub) (0.1.2)\nCollecting opentelemetry-semantic-conventions==0.57b0 (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.1.4->mlflow)\n  Downloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.4->mlflow) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.4->mlflow) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.4->mlflow) (0.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.4->mlflow) (3.4.2)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->dagshub) (1.1.0)\nRequirement already satisfied: multidict>=4.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.6->gql[requests]->dagshub) (6.6.3)\nRequirement already satisfied: propcache>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.6->gql[requests]->dagshub) (0.3.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3->mlflow) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3->mlflow) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3->mlflow) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3->mlflow) (2024.2.0)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.4->mlflow) (0.4.2)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.4->mlflow) (4.9.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3->mlflow) (2024.2.0)\nRequirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.4->mlflow) (0.6.1)\nDownloading dagshub-0.6.2-py3-none-any.whl (261 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.2/261.2 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading mlflow-3.1.4-py3-none-any.whl (24.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading mlflow_skinny-3.1.4-py3-none-any.whl (1.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\nDownloading dacite-1.6.0-py3-none-any.whl (12 kB)\nDownloading dagshub_annotation_converter-0.1.11-py3-none-any.whl (35 kB)\nDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pathvalidate-3.3.1-py3-none-any.whl (24 kB)\nDownloading treelib-1.8.0-py3-none-any.whl (30 kB)\nDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\nDownloading databricks_sdk-0.61.0-py3-none-any.whl (680 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m680.6/680.6 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\nDownloading opentelemetry_api-1.36.0-py3-none-any.whl (65 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_sdk-1.36.0-py3-none-any.whl (119 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opentelemetry_semantic_conventions-0.57b0-py3-none-any.whl (201 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading gql-3.5.3-py2.py3-none-any.whl (74 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: appdirs, treelib, pathvalidate, gunicorn, graphql-core, dacite, backoff, opentelemetry-api, graphql-relay, gql, opentelemetry-semantic-conventions, graphene, databricks-sdk, opentelemetry-sdk, mlflow-skinny, dagshub-annotation-converter, mlflow, dagshub\n  Attempting uninstall: dacite\n    Found existing installation: dacite 1.9.2\n    Uninstalling dacite-1.9.2:\n      Successfully uninstalled dacite-1.9.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nydata-profiling 4.16.1 requires dacite>=1.8, but you have dacite 1.6.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed appdirs-1.4.4 backoff-2.2.1 dacite-1.6.0 dagshub-0.6.2 dagshub-annotation-converter-0.1.11 databricks-sdk-0.61.0 gql-3.5.3 graphene-3.4.3 graphql-core-3.2.6 graphql-relay-3.2.0 gunicorn-23.0.0 mlflow-3.1.4 mlflow-skinny-3.1.4 opentelemetry-api-1.36.0 opentelemetry-sdk-1.36.0 opentelemetry-semantic-conventions-0.57b0 pathvalidate-3.3.1 treelib-1.8.0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                                       \u001b[1m❗❗❗ AUTHORIZATION REQUIRED ❗❗❗\u001b[0m                                        \n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                       <span style=\"font-weight: bold\">❗❗❗ AUTHORIZATION REQUIRED ❗❗❗</span>                                        \n</pre>\n"},"metadata":{}},{"name":"stdout","text":"\n\nOpen the following link in your browser to authorize the client:\nhttps://dagshub.com/login/oauth/authorize?state=71bd3263-520b-471f-9c5b-f7fa61ea96c9&client_id=32b60ba385aa7cecf24046d8195a71c07dd345d9657977863b52e7748e0f0f28&middleman_request_id=ea1ddee3ffa93b6d48eb81aa0664c5b06528323a8653d248208f7bba6d912348\n\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Accessing as TamariToradze\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as TamariToradze\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Initialized MLflow to track repo \u001b[32m\"TamariToradze/ML-Final\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"TamariToradze/ML-Final\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Repository TamariToradze/ML-Final initialized!\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository TamariToradze/ML-Final initialized!\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Features Shape: (8190, 12)\nStores Shape: (45, 3)\nTrain Shape: (421570, 5)\nTest Shape: (115064, 4)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Data preprocessing\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import LabelEncoder\nimport pandas as pd\nimport numpy as np\n\nclass EnhancedDataProcessor(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Advanced data preprocessing pipeline for sales forecasting with time series features.\n    \n    This transformer handles merging store and feature data, creates lag and rolling window\n    features, and generates enhanced derived features for machine learning models.\n    \"\"\"\n    \n    def __init__(self, store_df, feature_df, lag_periods=[1,2,4,8,12], window_sizes=[4,8,12]):\n        \"\"\"\n        Initialize the preprocessor with configuration parameters.\n        \n        Args:\n            store_data: DataFrame containing store information\n            feature_df: DataFrame containing temporal features\n            lag_periods: List of lag periods for time series features\n            window_sizes: List of rolling window sizes\n        \"\"\"\n        self.store_data = store_df\n        self.feature_df = feature_df\n        self.lag_periods = lag_periods\n        self.window_sizes = window_sizes\n        \n        # Initialize state variables\n        self.type_encoder = None\n        self.historical_lag_stats = {}\n        self.historical_rolling_stats = {}\n\n    def fit(self, X, y=None):\n        \"\"\"\n        Fit the preprocessor on training data and compute historical statistics.\n        \n        Args:\n            X: Input features DataFrame\n            y: Target variable (required for computing historical statistics)\n        \"\"\"\n        if y is None:\n            raise ValueError(\"Target variable y must be provided for fitting\")\n            \n        # Create working copy and apply initial preprocessing\n        X_work = X.copy()\n        X_work = self._merge_external_data(X_work)\n        \n        # Fit and store label encoder for store types\n        self.type_encoder = LabelEncoder()\n        self.type_encoder.fit(X_work['Type'])\n        X_work['Type_encoded'] = self.type_encoder.transform(X_work['Type'])\n        \n        # Extract temporal features\n        X_work = self._extract_temporal_features(X_work)\n        \n        # Add target and sort for time series operations\n        X_work['Target'] = y\n        X_work = X_work.sort_values(['Store', 'Dept', 'Date']).reset_index(drop=True)\n        \n        # Compute and store historical statistics\n        self._fit_lag_statistics(X_work)\n        self._fit_rolling_statistics(X_work)\n\n        return self\n\n    def transform(self, X):\n        \"\"\"\n        Transform input data using fitted statistics and feature engineering.\n        \n        Args:\n            X: Input features DataFrame\n            \n        Returns:\n            Transformed feature matrix ready for ML models\n        \"\"\"\n        X_work = X.copy()\n        \n        # Apply core preprocessing steps\n        X_work = self._merge_external_data(X_work)\n        X_work['Type_encoded'] = self.type_encoder.transform(X_work['Type'])\n        X_work = self._extract_temporal_features(X_work)\n        X_work = X_work.sort_values(['Store', 'Dept', 'Date']).reset_index(drop=True)\n        \n        # Create time series features based on data availability\n        has_target = 'Weekly_Sales' in X_work.columns\n        \n        if has_target:\n            X_work = self._generate_lag_features_with_target(X_work)\n            X_work = self._generate_rolling_features_with_target(X_work)\n        else:\n            X_work = self._generate_lag_features_from_history(X_work)\n            X_work = self._generate_rolling_features_from_history(X_work)\n        \n        # Add enhanced derived features\n        X_work = self._create_enhanced_features(X_work)\n        \n        # Clean up and finalize\n        drop_columns = ['Date', 'Type']\n        if 'Weekly_Sales' in X_work.columns:\n            drop_columns.append('Weekly_Sales')\n        if 'Target' in X_work.columns:\n            drop_columns.append('Target')\n            \n        X_work = X_work.drop([col for col in drop_columns if col in X_work.columns], axis=1)\n        X_work = X_work.fillna(0)\n        \n        return X_work\n\n    def _merge_external_data(self, df):\n        \"\"\"Merge store and feature data with main dataset.\"\"\"\n        df = df.merge(self.store_data, on='Store', how='left')\n        df = df.merge(self.feature_df, on=['Store', 'Date'], how='left')\n\n        # Convert date column\n        df['Date'] = pd.to_datetime(df['Date'])\n\n        # Handle duplicate holiday columns from merging\n        if 'IsHoliday_x' in df.columns and 'IsHoliday_y' in df.columns:\n            df['IsHoliday'] = df['IsHoliday_y'].fillna(df['IsHoliday_x'])\n            df = df.drop(['IsHoliday_x', 'IsHoliday_y'], axis=1)\n\n        # Fill missing values in numerical columns\n        numerical_features = ['CPI', 'Fuel_Price', 'Unemployment', 'Temperature']\n        self._handle_missing_numericals(numerical_features, df)\n\n        # Handle markdown features\n        markdown_features = [col for col in df.columns if 'MarkDown' in col]\n        for col in markdown_features:\n            df[col] = df[col].fillna(0)\n\n        # Handle boolean holiday feature\n        df['IsHoliday'] = df['IsHoliday'].fillna(False)\n\n        # Remove columns with excessive missing values\n        high_missing_cols = ['MarkDown1', 'MarkDown3', 'MarkDown5']\n        df = df.drop([col for col in high_missing_cols if col in df.columns], axis=1)\n\n        return df\n\n    def _handle_missing_numericals(self, columns, data):\n        \"\"\"Fill missing values in numerical columns using median imputation.\"\"\"\n        for col in columns:\n            if col in data.columns:\n                missing_count = data[col].isnull().sum()\n                if missing_count > 0:\n                    data[col] = data[col].fillna(data[col].median())\n\n    def _extract_temporal_features(self, df):\n        \"\"\"Extract comprehensive time-based features from date column.\"\"\"\n        df = df.copy()\n        \n        # Basic time components\n        df['Day'] = df['Date'].dt.day.astype('int64')\n        df['Week'] = df['Date'].dt.isocalendar().week.astype('int64')\n        df['Month'] = df['Date'].dt.month.astype('int64')\n        df['Year'] = df['Date'].dt.year.astype('int64')\n\n        # Advanced time features\n        df['Quarter'] = df['Date'].dt.quarter.astype('int64')\n        df['DayOfYear'] = df['Date'].dt.dayofyear.astype('int64')\n        df['WeekOfMonth'] = ((df['Date'].dt.day - 1) // 7 + 1).astype('int64')\n        \n        # Seasonal indicators\n        df['IsDecember'] = (df['Month'] == 12).astype(int)\n        df['IsQ4'] = (df['Quarter'] == 4).astype(int)\n\n        return df\n\n    def _fit_lag_statistics(self, df):\n        \"\"\"Compute and store lag-based statistics from training data.\"\"\"\n        for lag in self.lag_periods:\n            lag_stats = df.groupby(['Store', 'Dept'])['Target'].mean()\n            self.historical_lag_stats[f'lag_{lag}'] = lag_stats.to_dict()\n\n    def _fit_rolling_statistics(self, df):\n        \"\"\"Compute and store rolling window statistics from training data.\"\"\"\n        for window in self.window_sizes:\n            # Rolling mean statistics\n            rolling_means = (df.groupby(['Store', 'Dept'])['Target']\n                           .rolling(window=window, min_periods=1)\n                           .mean()\n                           .groupby(['Store', 'Dept'])\n                           .last())\n            \n            self.historical_rolling_stats[f'rolling_mean_{window}'] = rolling_means.to_dict()\n            \n            # Rolling standard deviation statistics\n            rolling_stds = (df.groupby(['Store', 'Dept'])['Target']\n                          .rolling(window=window, min_periods=1)\n                          .std()\n                          .groupby(['Store', 'Dept'])\n                          .last())\n            self.historical_rolling_stats[f'rolling_std_{window}'] = rolling_stds.to_dict()\n\n    def _generate_lag_features_with_target(self, df):\n        \"\"\"Generate lag features using actual target values.\"\"\"\n        for lag in self.lag_periods:\n            # Create lag feature\n            lag_values = df.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(lag)\n            group_means = df.groupby(['Store', 'Dept'])['Weekly_Sales'].transform('mean')\n            lag_values = lag_values.fillna(group_means)\n            \n            df[f'lag_{lag}'] = lag_values\n            df[f'growth_{lag}w'] = df['Weekly_Sales'] / (df[f'lag_{lag}'] + 1)\n            \n        return df\n\n    def _generate_rolling_features_with_target(self, df):\n        \"\"\"Generate rolling window features using actual target values.\"\"\"\n        for window in self.window_sizes:\n            # Rolling statistics\n            rolling_mean = (df.groupby(['Store', 'Dept'])['Weekly_Sales']\n                          .rolling(window=window, min_periods=1)\n                          .mean()\n                          .reset_index(level=[0, 1], drop=True))\n            \n            rolling_std = (df.groupby(['Store', 'Dept'])['Weekly_Sales']\n                         .rolling(window=window, min_periods=1)\n                         .std()\n                         .reset_index(level=[0, 1], drop=True)\n                         .fillna(0))\n\n            df[f'rolling_mean_{window}'] = rolling_mean.values\n            df[f'rolling_std_{window}'] = rolling_std.values\n            df[f'rolling_cv_{window}'] = df[f'rolling_std_{window}'] / (df[f'rolling_mean_{window}'] + 1)\n            df[f'sales_vs_mean_{window}'] = df['Weekly_Sales'] / (df[f'rolling_mean_{window}'] + 1)\n            \n        return df\n\n    def _generate_lag_features_from_history(self, df):\n        \"\"\"Generate lag features using historical statistics for prediction.\"\"\"\n        for lag in self.lag_periods:\n            if f'lag_{lag}' in self.historical_lag_stats:\n                lag_values = []\n                for _, row in df.iterrows():\n                    key = (row['Store'], row['Dept'])\n                    value = self.historical_lag_stats[f'lag_{lag}'].get(key, 15000)\n                    lag_values.append(value)\n                \n                df[f'lag_{lag}'] = lag_values\n                df[f'growth_{lag}w'] = 1.0\n            else:\n                df[f'lag_{lag}'] = 10000\n                df[f'growth_{lag}w'] = 1.0\n                \n        return df\n\n    def _generate_rolling_features_from_history(self, df):\n        \"\"\"Generate rolling features using historical statistics for prediction.\"\"\"\n        for window in self.window_sizes:\n            if f'rolling_mean_{window}' in self.historical_rolling_stats:\n                rolling_values = []\n                for _, row in df.iterrows():\n                    key = (row['Store'], row['Dept'])\n                    value = self.historical_rolling_stats[f'rolling_mean_{window}'].get(key, 15000)\n                    rolling_values.append(value)\n                \n                df[f'rolling_mean_{window}'] = rolling_values\n                df[f'rolling_std_{window}'] = 0\n                df[f'rolling_cv_{window}'] = 0\n                df[f'sales_vs_mean_{window}'] = 1.0\n            else:\n                df[f'rolling_mean_{window}'] = 10000\n                df[f'rolling_std_{window}'] = 0\n                df[f'rolling_cv_{window}'] = 0\n                df[f'sales_vs_mean_{window}'] = 1.0\n                \n        return df\n\n    def _create_enhanced_features(self, df):\n        \"\"\"Create advanced derived features for improved model performance.\"\"\"\n        \n        # Markdown aggregation features\n        markdown_cols = [col for col in df.columns if 'MarkDown' in col]\n        if markdown_cols:\n            df['total_markdown'] = df[markdown_cols].sum(axis=1)\n            df['markdown_count'] = (df[markdown_cols] > 0).sum(axis=1)\n            df['markdown_per_dept'] = df['total_markdown'] / (df['Dept'] + 1)\n\n        # Store-Department interaction features\n        if 'Store' in df.columns and 'Dept' in df.columns:\n            store_dept_combo = df['Store'].astype(str) + '_' + df['Dept'].astype(str)\n            combo_encoder = LabelEncoder()\n            df['Store_Dept_encoded'] = combo_encoder.fit_transform(store_dept_combo.fillna('NaN'))\n            \n            if 'Size' in df.columns:\n                df['store_dept_size_ratio'] = df['Size'] / (df['Dept'] + 1)\n\n        # Holiday interaction features\n        if 'IsHoliday' in df.columns:\n            df['holiday_weight'] = df['IsHoliday'].map({True: 5, False: 1})\n            df['holiday_dept_interaction'] = df['IsHoliday'].astype(int) * df['Dept']\n            \n            if 'Type_encoded' in df.columns:\n                df['holiday_type_interaction'] = df['IsHoliday'].astype(int) * df['Type_encoded']\n\n        # Economic pressure features\n        if 'CPI' in df.columns and 'Unemployment' in df.columns:\n            df['economic_pressure'] = df['CPI'] * df['Unemployment']\n            df['cpi_unemployment_ratio'] = df['CPI'] / (df['Unemployment'] + 0.1)\n\n        # Temperature-based features\n        if 'Temperature' in df.columns:\n            df['temp_squared'] = df['Temperature'] ** 2\n            df['is_cold'] = (df['Temperature'] < 50).astype(int)\n            df['is_hot'] = (df['Temperature'] > 80).astype(int)\n\n        # Fuel cost impact feature\n        if 'Fuel_Price' in df.columns and 'Size' in df.columns:\n            df['fuel_cost_impact'] = df['Fuel_Price'] * df['Size'] / 100000\n\n        # Type-size interaction\n        if 'Type_encoded' in df.columns and 'Size' in df.columns:\n            df['type_size_interaction'] = df['Type_encoded'] * df['Size'] / 1000\n\n        return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T09:19:30.585903Z","iopub.execute_input":"2025-08-01T09:19:30.586245Z","iopub.status.idle":"2025-08-01T09:19:31.744642Z","shell.execute_reply.started":"2025-08-01T09:19:30.586216Z","shell.execute_reply":"2025-08-01T09:19:31.743750Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def wmae_score(y_true, y_pred, weights):\n    return np.sum(weights * np.abs(y_true - y_pred)) / np.sum(weights)\n\n# Data preparation\n# Convert date columns to datetime format for proper temporal processing\nfeatures_df['Date'] = pd.to_datetime(features_df['Date'])\ntrain_df['Date'] = pd.to_datetime(train_df['Date'])\ntest_df['Date'] = pd.to_datetime(test_df['Date'])\n\n# Store original dataset size for reference\noriginal_sample_count = len(train_df)\n\n# Filter out negative sales values and reset index\ntrain_df = train_df[train_df['Weekly_Sales'] >= 0].reset_index(drop=True)\n\n# Optional: Log the data cleaning impact\nprint(f\"Data cleaning: Removed {original_sample_count - len(train_df)} records with negative sales\")\nprint(f\"Final training dataset size: {len(train_df)} records\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T09:19:31.745435Z","iopub.execute_input":"2025-08-01T09:19:31.745968Z","iopub.status.idle":"2025-08-01T09:19:31.884897Z","shell.execute_reply.started":"2025-08-01T09:19:31.745943Z","shell.execute_reply":"2025-08-01T09:19:31.883901Z"}},"outputs":[{"name":"stdout","text":"Data cleaning: Removed 1285 records with negative sales\nFinal training dataset size: 420285 records\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Temporal Cross Validation\ndef temporal_cross_validation_split(train_data, n_splits=5):\n    training_dataset=train_data.sort_values('Date')\n    dates=train_data['Date'].unique()\n    dates=np.sort(dates)\n    chunk_size=len(dates)//(n_splits+1)\n    fold_results=[]\n    for i in range(n_splits):\n        train_end_idx=(i+1) * chunk_size\n        val_start_idx=train_end_idx\n        val_end_idx=train_end_idx+chunk_size\n        train_dates=dates[:train_end_idx]\n        val_dates=dates[val_start_idx:val_end_idx]\n        train_idx=training_dataset[training_dataset['Date'].isin(train_dates)].index\n        val_idx=training_dataset[training_dataset['Date'].isin(val_dates)].index\n        fold_results.append((train_idx, val_idx))\n    return fold_results\n\n# Import required libraries\nimport pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.preprocessing import LabelEncoder\nimport joblib\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T09:19:31.886985Z","iopub.execute_input":"2025-08-01T09:19:31.887256Z","iopub.status.idle":"2025-08-01T09:19:36.750572Z","shell.execute_reply.started":"2025-08-01T09:19:31.887234Z","shell.execute_reply":"2025-08-01T09:19:36.749655Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Cleaning","metadata":{}},{"cell_type":"code","source":"# Create or set experiment for LightGBM\nexperiment_name = \"LightGBM_Training\"\nmlflow.set_experiment(experiment_name)\n\n# Data Cleaning Run\nwith mlflow.start_run(run_name=\"LightGBM_Cleaning\"):\n    # Log data cleaning metrics\n    mlflow.log_metric(\"original_samples\", original_sample_count)\n    mlflow.log_metric(\"cleaned_samples\", len(train_df))\n    mlflow.log_metric(\"removed_samples\", original_sample_count - len(train_df))\n    mlflow.log_metric(\"removal_percentage\", (original_sample_count - len(train_df)) / original_sample_count * 100)\n    \n    # Log data shapes\n    mlflow.log_metric(\"features_shape_rows\", features_df.shape[0])\n    mlflow.log_metric(\"features_shape_cols\", features_df.shape[1])\n    mlflow.log_metric(\"train_shape_rows\", train_df.shape[0])\n    mlflow.log_metric(\"train_shape_cols\", train_df.shape[1])\n    mlflow.log_metric(\"test_shape_rows\", test_df.shape[0])\n    mlflow.log_metric(\"test_shape_cols\", test_df.shape[1])\n    \n    # Log data cleaning parameters\n    mlflow.log_param(\"negative_sales_removed\", True)\n    mlflow.log_param(\"date_columns_converted\", True)\n    \n    print(\"✓ Data cleaning run logged to MLflow\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T09:19:36.751636Z","iopub.execute_input":"2025-08-01T09:19:36.752398Z","iopub.status.idle":"2025-08-01T09:20:01.885107Z","shell.execute_reply.started":"2025-08-01T09:19:36.752356Z","shell.execute_reply":"2025-08-01T09:20:01.884229Z"}},"outputs":[{"name":"stdout","text":"✓ Data cleaning run logged to MLflow\n🏃 View run LightGBM_Cleaning at: https://dagshub.com/TamariToradze/ML-Final.mlflow/#/experiments/17/runs/449257015f1d43e4b3809efe8ea60006\n🧪 View experiment at: https://dagshub.com/TamariToradze/ML-Final.mlflow/#/experiments/17\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"code","source":"# Feature Engineering Run\nwith mlflow.start_run(run_name=\"LightGBM_Feature_Engineering\"):\n    # Log feature engineering parameters\n    lag_periods = [1, 2, 4, 8, 12]\n    window_sizes = [4, 8, 12]\n    \n    mlflow.log_param(\"lag_periods\", lag_periods)\n    mlflow.log_param(\"window_sizes\", window_sizes)\n    mlflow.log_param(\"temporal_features_extracted\", True)\n    mlflow.log_param(\"store_dept_interaction\", True)\n    mlflow.log_param(\"holiday_interactions\", True)\n    mlflow.log_param(\"economic_features\", True)\n    mlflow.log_param(\"temperature_features\", True)\n    \n    # Log feature counts\n    sample_processor = EnhancedDataProcessor(\n        store_df=stores_df,\n        feature_df=features_df,\n        lag_periods=lag_periods,\n        window_sizes=window_sizes\n    )\n    \n    # Sample transformation to count features\n    X_sample = train_df.drop('Weekly_Sales', axis=1).head(100)\n    y_sample = train_df['Weekly_Sales'].head(100)\n    X_transformed = sample_processor.fit(X_sample, y_sample).transform(X_sample)\n    \n    mlflow.log_metric(\"total_features_created\", X_transformed.shape[1])\n    mlflow.log_metric(\"lag_features_count\", len(lag_periods) * 2)  # lag + growth features\n    mlflow.log_metric(\"rolling_features_count\", len(window_sizes) * 4)  # mean, std, cv, ratio\n    \n    print(\"✓ Feature engineering run logged to MLflow\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T09:20:01.886100Z","iopub.execute_input":"2025-08-01T09:20:01.886390Z","iopub.status.idle":"2025-08-01T09:20:20.882333Z","shell.execute_reply.started":"2025-08-01T09:20:01.886366Z","shell.execute_reply":"2025-08-01T09:20:20.881425Z"}},"outputs":[{"name":"stdout","text":"✓ Feature engineering run logged to MLflow\n🏃 View run LightGBM_Feature_Engineering at: https://dagshub.com/TamariToradze/ML-Final.mlflow/#/experiments/17/runs/7baaad47ced147e695af5a1be0f1aee2\n🧪 View experiment at: https://dagshub.com/TamariToradze/ML-Final.mlflow/#/experiments/17\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"markdown","source":"ჰიპერპარამეტრების ოპტიმიზაცია ხდება აქ","metadata":{}},{"cell_type":"code","source":"import optuna\ndef hyperparameter_optimization():\n    def objective(trial):\n        # Dynamic parameter suggestions\n        params = {\n            'objective': 'regression',\n            'metric': 'mae',\n            'boosting_type': 'gbdt',\n            'num_leaves': trial.suggest_int('num_leaves', 10, 300),\n            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n            'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n            'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n            'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n            'max_depth': trial.suggest_int('max_depth', 3, 15),\n            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n            'reg_alpha': trial.suggest_float('reg_alpha', 0, 2.0),\n            'reg_lambda': trial.suggest_float('reg_lambda', 0, 2.0),\n            'verbose': -1,\n            'random_state': 42,\n            'n_estimators': 2000\n        }\n        \n        _, avg_wmae = perform_cv_with_mlflow_optuna(train_df, params)\n        return avg_wmae\n    \n    study = optuna.create_study(direction='minimize')\n    study.optimize(objective, n_trials=50)  \n    \n    return study.best_params, study.best_value\n\ndef perform_cv_with_mlflow_optuna(training_data, params):\n   \n    input_features = [col for col in training_data.columns if col != 'Weekly_Sales']\n    X = training_data[input_features]\n    y = training_data['Weekly_Sales']\n    \n    sample_weights = training_data['IsHoliday'].map({True: 5, False: 1}).values\n    folds = temporal_cross_validation_split(training_data, n_splits=5)\n    \n    weighted_mae_list = []\n    \n    for i, (train_idx, val_idx) in enumerate(folds):\n        X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n        X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n        val_weights = sample_weights[val_idx]\n        val_data = training_data.iloc[val_idx]\n        \n        preproc = EnhancedDataProcessor(\n            store_df=stores_df,\n            feature_df=features_df,\n            lag_periods=[1, 2, 4, 8, 12],\n            window_sizes=[4, 8, 12]\n        )\n        \n        X_train_prepared = preproc.fit(X_train, y_train).transform(X_train)\n        X_val_prepared = preproc.transform(val_data)\n        \n        if 'Weekly_Sales' in X_val_prepared.columns:\n            X_val_prepared = X_val_prepared.drop(columns='Weekly_Sales')\n        \n        model = lgb.LGBMRegressor(**params)\n        model.fit(\n            X_train_prepared,\n            y_train,\n            eval_set=[(X_val_prepared, y_val)],\n            callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)]\n        )\n        \n        preds = model.predict(X_val_prepared)\n        preds = np.maximum(preds, 0)\n        \n        fold_wmae = wmae_score(y_val, preds, val_weights)\n        weighted_mae_list.append(fold_wmae)\n    \n    return weighted_mae_list, np.mean(weighted_mae_list)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def perform_cv_with_mlflow(training_data):\n    with mlflow.start_run(run_name=\"LightGBM_CrossValidation\"):\n        # LightGBM hyperparameters\n        params = {\n            'objective': 'regression',\n            'metric': 'mae',\n            'boosting_type': 'gbdt',\n            'num_leaves': 63,\n            'learning_rate': 0.03,\n            'feature_fraction': 0.8,\n            'bagging_fraction': 0.8,\n            'bagging_freq': 5,\n            'max_depth': 8,\n            'min_child_samples': 20,\n            'reg_alpha': 0.1,\n            'reg_lambda': 0.1,\n            'verbose': -1,\n            'random_state': 42,\n            'n_estimators': 2000\n        }\n        \n        # Log hyperparameters\n        for param, value in params.items():\n            mlflow.log_param(param, value)\n        \n        mlflow.log_param(\"cv_folds\", 5)\n        mlflow.log_param(\"cv_strategy\", \"temporal\")\n        mlflow.log_param(\"holiday_weighting\", True)\n\n        # Identify input features\n        input_features = [col for col in training_data.columns if col != 'Weekly_Sales']\n        X = training_data[input_features]\n        y = training_data['Weekly_Sales']\n\n        # Assign higher weights to holiday weeks\n        sample_weights = training_data['IsHoliday'].map({True: 5, False: 1}).values\n\n        # Get cross-validation splits\n        folds = temporal_cross_validation_split(training_data, n_splits=5)\n\n        fold_metrics = []\n        weighted_mae_list = []\n\n        for i, (train_idx, val_idx) in enumerate(folds):\n            X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n            X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n            val_weights = sample_weights[val_idx]\n\n            val_data = training_data.iloc[val_idx]\n\n            # Initialize the preprocessing pipeline\n            preproc = EnhancedDataProcessor(\n                store_df=stores_df,\n                feature_df=features_df,\n                lag_periods=[1, 2, 4, 8, 12],\n                window_sizes=[4, 8, 12]\n            )\n\n            X_train_prepared = preproc.fit(X_train, y_train).transform(X_train)\n            X_val_prepared = preproc.transform(val_data)\n\n            # Remove target if accidentally included\n            if 'Weekly_Sales' in X_val_prepared.columns:\n                X_val_prepared = X_val_prepared.drop(columns='Weekly_Sales')\n\n            # Initialize and train the model\n            model = lgb.LGBMRegressor(**params)\n            model.fit(\n                X_train_prepared,\n                y_train,\n                eval_set=[(X_val_prepared, y_val)],\n                callbacks=[lgb.early_stopping(100), lgb.log_evaluation(0)]\n            )\n\n            preds = model.predict(X_val_prepared)\n            preds = np.maximum(preds, 0)  # Ensure no negative predictions\n\n            # Evaluate metrics\n            fold_mae = mean_absolute_error(y_val, preds)\n            fold_rmse = np.sqrt(mean_squared_error(y_val, preds))\n            fold_r2 = r2_score(y_val, preds)\n            fold_wmae = wmae_score(y_val, preds, val_weights)\n\n            fold_metrics.append({\n                'fold': i,\n                'mae': fold_mae,\n                'rmse': fold_rmse,\n                'r2': fold_r2,\n                'wmae': fold_wmae\n            })\n            weighted_mae_list.append(fold_wmae)\n\n            # Log fold metrics\n            mlflow.log_metric(f\"fold_{i}_mae\", fold_mae)\n            mlflow.log_metric(f\"fold_{i}_rmse\", fold_rmse)\n            mlflow.log_metric(f\"fold_{i}_r2\", fold_r2)\n            mlflow.log_metric(f\"fold_{i}_wmae\", fold_wmae)\n\n            print(f\"Fold {i}: MAE={fold_mae:.2f}, RMSE={fold_rmse:.2f}, R²={fold_r2:.4f}, WMAE={fold_wmae:.2f}\")\n\n        # Aggregate CV performance\n        mean_mae = np.mean([m['mae'] for m in fold_metrics])\n        mean_rmse = np.mean([m['rmse'] for m in fold_metrics])\n        mean_r2 = np.mean([m['r2'] for m in fold_metrics])\n        mean_wmae = np.mean(weighted_mae_list)\n        \n        # Log aggregate metrics\n        mlflow.log_metric(\"cv_mean_mae\", mean_mae)\n        mlflow.log_metric(\"cv_mean_rmse\", mean_rmse)\n        mlflow.log_metric(\"cv_mean_r2\", mean_r2)\n        mlflow.log_metric(\"cv_mean_wmae\", mean_wmae)\n        \n        # Log standard deviations\n        mlflow.log_metric(\"cv_std_mae\", np.std([m['mae'] for m in fold_metrics]))\n        mlflow.log_metric(\"cv_std_rmse\", np.std([m['rmse'] for m in fold_metrics]))\n        mlflow.log_metric(\"cv_std_r2\", np.std([m['r2'] for m in fold_metrics]))\n        mlflow.log_metric(\"cv_std_wmae\", np.std(weighted_mae_list))\n\n        print(\"\\nCross-validation summary:\")\n        print(f\"Avg MAE: {mean_mae:.2f}\")\n        print(f\"Avg RMSE: {mean_rmse:.2f}\")\n        print(f\"Avg R²: {mean_r2:.4f}\")\n        print(f\"Avg WMAE: {mean_wmae:.2f}\")\n        \n        print(\"✓ Cross-validation run logged to MLflow\")\n\n        return fold_metrics, mean_wmae","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T09:20:20.883378Z","iopub.execute_input":"2025-08-01T09:20:20.883655Z","iopub.status.idle":"2025-08-01T09:20:20.900780Z","shell.execute_reply.started":"2025-08-01T09:20:20.883633Z","shell.execute_reply":"2025-08-01T09:20:20.899870Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Final Model Training and Registration Run\ndef build_and_train_final_model(dataframe):\n    with mlflow.start_run(run_name=\"LightGBM_Final_Training\"):\n        # Define optimized LightGBM parameters\n        lgbm_config = {\n            'objective': 'regression',\n            'metric': 'mae',\n            'boosting_type': 'gbdt',\n            'num_leaves': 63,\n            'learning_rate': 0.03,\n            'feature_fraction': 0.8,\n            'bagging_fraction': 0.8,\n            'bagging_freq': 5,\n            'max_depth': 8,\n            'min_child_samples': 20,\n            'reg_alpha': 0.1,\n            'reg_lambda': 0.1,\n            'verbose': -1,\n            'random_state': 42,\n            'n_estimators': 2000\n        }\n        \n        # Log parameters\n        for param, value in lgbm_config.items():\n            mlflow.log_param(param, value)\n\n        # Create preprocessing object\n        preprocessor = EnhancedDataProcessor(\n            store_df=stores_df,\n            feature_df=features_df,\n            lag_periods=[1, 2, 4, 8, 12],\n            window_sizes=[4, 8, 12]\n        )\n\n        # Assemble the pipeline\n        pipeline = Pipeline([\n            ('preprocessor', preprocessor),\n            ('model', lgb.LGBMRegressor(**lgbm_config))\n        ])\n\n        # Separate features and target\n        predictors = [col for col in dataframe.columns if col != 'Weekly_Sales']\n        X = dataframe[predictors]\n        y = dataframe['Weekly_Sales']\n\n        # Train the pipeline\n        pipeline.fit(X, y)\n\n        # Predictions on training data\n        predictions = pipeline.predict(X)\n        predictions = np.maximum(predictions, 0)  # Ensure no negative sales\n\n        # Evaluate performance\n        error_mae = mean_absolute_error(y, predictions)\n        error_rmse = np.sqrt(mean_squared_error(y, predictions))\n        error_r2 = r2_score(y, predictions)\n        \n        # Calculate WMAE\n        sample_weights = dataframe['IsHoliday'].map({True: 5, False: 1}).values\n        error_wmae = wmae_score(y, predictions, sample_weights)\n\n        # Log training metrics\n        mlflow.log_metric(\"train_mae\", error_mae)\n        mlflow.log_metric(\"train_rmse\", error_rmse)\n        mlflow.log_metric(\"train_r2\", error_r2)\n        mlflow.log_metric(\"train_wmae\", error_wmae)\n        \n        # Log dataset info\n        mlflow.log_metric(\"training_samples\", len(X))\n        mlflow.log_metric(\"n_features\", len(predictors))\n\n        print(\"Training performance:\")\n        print(f\"MAE: {error_mae:.2f}\")\n        print(f\"RMSE: {error_rmse:.2f}\")\n        print(f\"R²: {error_r2:.4f}\")\n        print(f\"WMAE: {error_wmae:.2f}\")\n        \n        # Save pipeline locally instead of using mlflow.sklearn.log_model\n        model_path = 'lightgbm_pipeline.pkl'\n        joblib.dump(pipeline, model_path)\n        \n        # Log the pickle file as artifact (DagsHub compatible)\n        mlflow.log_artifact(model_path)\n        \n        # Log additional model metadata as params\n        mlflow.log_param(\"model_file\", model_path)\n        mlflow.log_param(\"model_type\", \"sklearn_pipeline\")\n        mlflow.log_param(\"preprocessing_included\", True)\n        mlflow.log_param(\"model_architecture\", \"LightGBM\")\n        \n        print(\"✓ Final model training run logged to MLflow\")\n        print(\"✓ Model saved as pickle artifact (DagsHub compatible)\")\n\n        return pipeline","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T09:20:20.901716Z","iopub.execute_input":"2025-08-01T09:20:20.901985Z","iopub.status.idle":"2025-08-01T09:20:20.922173Z","shell.execute_reply.started":"2025-08-01T09:20:20.901964Z","shell.execute_reply":"2025-08-01T09:20:20.921141Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Prepare data\nX_train = train_df.drop('Weekly_Sales', axis=1)\ny_train = train_df['Weekly_Sales']\n\nprint(f\"Training data shape: {X_train.shape}\")\nprint(f\"Target shape: {y_train.shape}\")\n\n# Run cross-validation with MLflow logging\ncv_scores, avg_wmae = perform_cv_with_mlflow(train_df)\n\n# Build and train final model with MLflow logging\nmy_pipeline = build_and_train_final_model(train_df)\n\n# Prediction Run\nwith mlflow.start_run(run_name=\"LightGBM_Prediction\"):\n    # Make predictions on test set\n    test_predictions = my_pipeline.predict(test_df)\n    test_predictions = np.maximum(test_predictions, 0)\n    \n    # Log prediction info\n    mlflow.log_metric(\"test_samples\", len(test_df))\n    mlflow.log_metric(\"min_prediction\", np.min(test_predictions))\n    mlflow.log_metric(\"max_prediction\", np.max(test_predictions))\n    mlflow.log_metric(\"mean_prediction\", np.mean(test_predictions))\n    mlflow.log_metric(\"std_prediction\", np.std(test_predictions))\n    \n    # Create submission file\n    submission = test_df[['Store', 'Dept', 'Date']].copy()\n    submission['Weekly_Sales'] = test_predictions\n    \n    submission['Date_str'] = pd.to_datetime(submission['Date']).dt.strftime('%Y-%m-%d')\n    submission['Id'] = (submission['Store'].astype(str) + '_' + \n                       submission['Dept'].astype(str) + '_' + \n                       submission['Date_str'])\n    final_submission = submission[['Id', 'Weekly_Sales']].copy()\n    \n    # Save submission\n    final_submission.to_csv('submission.csv', index=False)\n    mlflow.log_artifact('submission.csv')\n    \n    print(\"✓ Prediction run logged to MLflow\")\n    print(\"✓ Submission file created and logged\")\n\n# Model Comparison and Best Model Selection Run\nwith mlflow.start_run(run_name=\"LightGBM_Model_Selection\"):\n    # Log final model selection metrics\n    mlflow.log_metric(\"final_cv_wmae\", avg_wmae)\n    mlflow.log_param(\"model_architecture\", \"LightGBM\")\n    mlflow.log_param(\"pipeline_components\", \"EnhancedDataProcessor + LGBMRegressor\")\n    mlflow.log_param(\"ready_for_production\", True)\n    \n    # Log model characteristics\n    mlflow.log_param(\"handles_raw_data\", True)\n    mlflow.log_param(\"preprocessing_included\", True)\n    mlflow.log_param(\"feature_engineering_included\", True)\n    mlflow.log_param(\"time_series_features\", True)\n    \n    print(\"✓ Model selection run logged to MLflow\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T09:20:20.923478Z","iopub.execute_input":"2025-08-01T09:20:20.923909Z","iopub.status.idle":"2025-08-01T09:36:53.866344Z","shell.execute_reply.started":"2025-08-01T09:20:20.923874Z","shell.execute_reply":"2025-08-01T09:36:53.865472Z"}},"outputs":[{"name":"stdout","text":"Training data shape: (420285, 4)\nTarget shape: (420285,)\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[2]\tvalid_0's l1: 15098.8\nFold 0: MAE=15098.83, RMSE=23202.30, R²=0.0001, WMAE=15665.83\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[1]\tvalid_0's l1: 15135.9\nFold 1: MAE=15135.90, RMSE=22604.59, R²=-0.0006, WMAE=14980.71\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[2]\tvalid_0's l1: 15008.9\nFold 2: MAE=15008.94, RMSE=21887.28, R²=-0.0003, WMAE=15055.80\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[1]\tvalid_0's l1: 15772.4\nFold 3: MAE=15772.36, RMSE=24701.07, R²=-0.0010, WMAE=16078.85\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[1]\tvalid_0's l1: 15199.8\nFold 4: MAE=15199.76, RMSE=22066.79, R²=-0.0002, WMAE=15265.65\n\nCross-validation summary:\nAvg MAE: 15243.16\nAvg RMSE: 22892.41\nAvg R²: -0.0004\nAvg WMAE: 15409.37\n✓ Cross-validation run logged to MLflow\n🏃 View run LightGBM_CrossValidation at: https://dagshub.com/TamariToradze/ML-Final.mlflow/#/experiments/17/runs/e91be9d39afa4b1ab3d283628ad6e1a9\n🧪 View experiment at: https://dagshub.com/TamariToradze/ML-Final.mlflow/#/experiments/17\nTraining performance:\nMAE: 1294.69\nRMSE: 2402.67\nR²: 0.9888\nWMAE: 1340.13\n✓ Final model training run logged to MLflow\n✓ Model saved as pickle artifact (DagsHub compatible)\n🏃 View run LightGBM_Final_Training at: https://dagshub.com/TamariToradze/ML-Final.mlflow/#/experiments/17/runs/97342c4589924bbca5d94ee0fe696c4c\n🧪 View experiment at: https://dagshub.com/TamariToradze/ML-Final.mlflow/#/experiments/17\n✓ Prediction run logged to MLflow\n✓ Submission file created and logged\n🏃 View run LightGBM_Prediction at: https://dagshub.com/TamariToradze/ML-Final.mlflow/#/experiments/17/runs/409b0ab8cd134be693f103b1729e819d\n🧪 View experiment at: https://dagshub.com/TamariToradze/ML-Final.mlflow/#/experiments/17\n✓ Model selection run logged to MLflow\n🏃 View run LightGBM_Model_Selection at: https://dagshub.com/TamariToradze/ML-Final.mlflow/#/experiments/17/runs/d8cbe7007ab74468a794ba730226321f\n🧪 View experiment at: https://dagshub.com/TamariToradze/ML-Final.mlflow/#/experiments/17\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"def save_best_model_info():\n    \"\"\"\n    Save best model information manually since DagsHub doesn't support Model Registry\n    \"\"\"\n    model_info = {\n        'model_name': 'LightGBM_Sales_Forecasting_Pipeline',\n        'experiment_name': experiment_name,\n        'cv_wmae': avg_wmae,\n        'model_type': 'LightGBM',\n        'pipeline_included': True,\n        'preprocessing_included': True,\n        'ready_for_production': True,\n        'model_path': 'lightgbm_pipeline.pkl'\n    }\n    \n    # Save model info to file\n    import json\n    with open('best_model_info.json', 'w') as f:\n        json.dump(model_info, f, indent=2)\n    \n    print(f\"✓ Best model information saved to best_model_info.json\")\n    print(f\"✓ Model WMAE: {avg_wmae:.4f}\")\n    \n    return model_info\n\n# Save best model information (DagsHub compatible)\nbest_model_info = save_best_model_info()\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"MLflow Pipeline Execution Complete!\")\nprint(\"=\"*80)\nprint(f\"Experiment: {experiment_name}\")\nprint(\"Runs created:\")\nprint(\"  1. LightGBM_Cleaning - Data cleaning and preparation\")\nprint(\"  2. LightGBM_Feature_Engineering - Feature engineering setup\")\nprint(\"  3. LightGBM_CrossValidation - Cross-validation results\")\nprint(\"  4. LightGBM_Final_Training - Final model training\")\nprint(\"  5. LightGBM_Prediction - Test set predictions\")\nprint(\"  6. LightGBM_Model_Selection - Model selection summary\")\nprint(\"=\"*80)\nprint(\"Model Information:\")\nprint(\"  - Model: LightGBM_Sales_Forecasting_Pipeline\")\nprint(\"  - Status: Saved as artifact (DagsHub compatible)\")\nprint(\"  - Pipeline: Complete preprocessing + LightGBM model\")\nprint(f\"  - WMAE: {avg_wmae:.4f}\")\nprint(\"  - Files: lightgbm_pipeline.pkl, best_model_info.json\")\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T09:36:53.868760Z","iopub.execute_input":"2025-08-01T09:36:53.868997Z","iopub.status.idle":"2025-08-01T09:36:53.878535Z","shell.execute_reply.started":"2025-08-01T09:36:53.868977Z","shell.execute_reply":"2025-08-01T09:36:53.877593Z"}},"outputs":[{"name":"stdout","text":"✓ Best model information saved to best_model_info.json\n✓ Model WMAE: 15409.3683\n\n================================================================================\nMLflow Pipeline Execution Complete!\n================================================================================\nExperiment: LightGBM_Training\nRuns created:\n  1. LightGBM_Cleaning - Data cleaning and preparation\n  2. LightGBM_Feature_Engineering - Feature engineering setup\n  3. LightGBM_CrossValidation - Cross-validation results\n  4. LightGBM_Final_Training - Final model training\n  5. LightGBM_Prediction - Test set predictions\n  6. LightGBM_Model_Selection - Model selection summary\n================================================================================\nModel Information:\n  - Model: LightGBM_Sales_Forecasting_Pipeline\n  - Status: Saved as artifact (DagsHub compatible)\n  - Pipeline: Complete preprocessing + LightGBM model\n  - WMAE: 15409.3683\n  - Files: lightgbm_pipeline.pkl, best_model_info.json\n================================================================================\n","output_type":"stream"}],"execution_count":10}]}